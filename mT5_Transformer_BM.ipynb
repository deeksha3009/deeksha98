{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deeksha3009/deeksha98/blob/master/mT5_Transformer_BM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets sacrebleu transformers"
      ],
      "metadata": {
        "id": "7vdbAkdKeRoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRuCj12ejMmw"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datasets\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, MT5Tokenizer, MT5ForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_metric\n",
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oj2xIzyBeoii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(file_name):\n",
        "  with open(file_name, 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "  return lines\n",
        "\n",
        "src_lines = load_data('/content/drive/MyDrive/Thesis/NLLB_hi_kn-hi.txt')\n",
        "tgt_lines = load_data('/content/drive/MyDrive/Thesis/NLLB_hi_kn-kn.txt')\n",
        "\n",
        "dataset = pd.DataFrame({'src': src_lines, 'tgt': tgt_lines})\n",
        "\n",
        "dataset = dataset.drop_duplicates().sample(frac=0.02).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "MfzP7AB-poc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_token_distribution(dataset, column, title):\n",
        "    token_lengths = dataset[column].apply(lambda x: len(x.split()))\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.hist(token_lengths, bins=30, alpha=0.7, color='blue')\n",
        "    plt.title(f'Token Distribution: {title}')\n",
        "    plt.xlabel('length of Tokens')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "plot_token_distribution(dataset, 'src', 'hindi Language - Tokens')\n",
        "plot_token_distribution(dataset, 'tgt', 'kannada Language - Tokens')\n"
      ],
      "metadata": {
        "id": "AXr6o6tyZnig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "def split_data(dataset):\n",
        "  train_data, test_data = train_test_split(dataset, test_size=0.25)\n",
        "  train_data, validation_data = train_test_split(train_data, test_size=0.35)\n",
        "  return train_data, validation_data, test_data\n",
        "\n",
        "train_data, validation_data, test_data = split_data(dataset)\n",
        "\n",
        "# Convert DataFrame to Dataset\n",
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "validation_dataset = Dataset.from_pandas(validation_data)\n",
        "test_dataset = Dataset.from_pandas(test_data)"
      ],
      "metadata": {
        "id": "T9oUVM73d7ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset.shape"
      ],
      "metadata": {
        "id": "7_zZb3WRpz_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer and model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = 'google/mt5-base'\n",
        "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
        "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "8t5gKDEheMFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "id": "WwbUYWwrXj6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = examples[\"src\"]\n",
        "    targets = examples[\"tgt\"]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "KXulfECtTT9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer on train dataset\")\n",
        "validation_dataset = validation_dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer on validation dataset\")\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer on test dataset\")\n"
      ],
      "metadata": {
        "id": "T-ahSHhqeKbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='./logs',\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=500,\n",
        "    eval_accumulation_steps=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Define the BLEU metric\n",
        "bleu_metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": bleu['score']}\n",
        "\n",
        "# Initialize the trainer for the initial training phase\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model initially\n"
      ],
      "metadata": {
        "id": "PrpNPMaveC0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "iQYe8t9v-2dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Extract the log history\n",
        "log_history = trainer.state.log_history\n",
        "\n",
        "# Convert the log history into a DataFrame\n",
        "metrics_df = pd.DataFrame(log_history)\n",
        "\n",
        "# Display the DataFrame to understand the structure\n",
        "metrics_df.head()\n"
      ],
      "metadata": {
        "id": "AWneDnULhG8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter out the steps that contain training and evaluation loss\n",
        "train_loss = metrics_df[metrics_df['loss'].notna()][['step', 'loss']]\n",
        "eval_loss = metrics_df[metrics_df['eval_loss'].notna()][['step', 'eval_loss']]\n",
        "\n",
        "# Plot the training loss\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(train_loss['step'], train_loss['loss'], label='Training Loss', color='yellow')\n",
        "\n",
        "# Plot the validation loss\n",
        "plt.plot(eval_loss['step'], eval_loss['eval_loss'], label='Validation Loss', color='red')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QEDTRvuLoJB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on the test set before fine-tuning\n",
        "initial_results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(f\"Initial BLEU score on test set: {initial_results['eval_bleu']}\")\n",
        "\n",
        "# Save the model checkpoint after initial training\n",
        "trainer.save_model(\"mt5_model.pt\")"
      ],
      "metadata": {
        "id": "WiKJQQ01_Utp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_translation(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = model.generate(**inputs).to(device)\n",
        "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "ZVNmR3hJT8ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Generate translations for the test set\n",
        "def get_score(test_dataset):\n",
        "  refs = [[ex[\"tgt\"]] for ex in test_dataset]\n",
        "  preds = [generate_translation(ex[\"src\"]) for ex in test_dataset]\n",
        "  bleu = sacrebleu.corpus_bleu(preds, refs)\n",
        "  print(f\"Test BLEU score after fine-tuning: {bleu.score}\")"
      ],
      "metadata": {
        "id": "YcPqMIcZGdJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_score(test_dataset)"
      ],
      "metadata": {
        "id": "M5SPUd5eRxXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"यह शुरू करने का समय है\"\n",
        "result = generate_translation(input_text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "3yrZy3TnG7B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back_dataset = pd.DataFrame({'src': tgt_lines , 'tgt': src_lines})\n",
        "\n",
        "back_dataset = back_dataset.drop_duplicates().sample(frac=0.02).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "syI9MnT4R7oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back_dataset.head(3)"
      ],
      "metadata": {
        "id": "ZIJTg58eR_gQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_train, rev_validation, rev_test = split_data(back_dataset)\n",
        "\n",
        "rev_train_dataset = Dataset.from_pandas(rev_train)\n",
        "rev_validation_dataset = Dataset.from_pandas(rev_validation)\n",
        "rev_test_dataset = Dataset.from_pandas(rev_test)"
      ],
      "metadata": {
        "id": "5AaGwnYoSpXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_train_dataset.shape"
      ],
      "metadata": {
        "id": "z47A-rgAS5b2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev_train_dataset = rev_train_dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer on train dataset back translation\")\n",
        "rev_validation_dataset = rev_validation_dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer on validation dataset back translation\")\n",
        "rev_test_dataset = rev_test_dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer on test dataset back translation\")"
      ],
      "metadata": {
        "id": "7dnvKFCfS8k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "0oPyqdz4TuaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",  # Save model at the end of each epoch\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='./logs',\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=500,\n",
        "    eval_accumulation_steps=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"bleu\",\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "\n",
        "# Define the BLEU metric\n",
        "bleu_metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    decoded_labels = [[label] for label in decoded_labels]\n",
        "    bleu = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    return {\"bleu\": bleu['score']}\n",
        "\n",
        "# Initialize the trainer for the initial training phase\n",
        "back_trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=rev_train_dataset,\n",
        "    eval_dataset=rev_validation_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model initially\n"
      ],
      "metadata": {
        "id": "fRPPz97LTbVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "back_trainer.train()"
      ],
      "metadata": {
        "id": "92pn4x4mTp7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Generate translations for the test set\n",
        "get_score(rev_test_dataset)"
      ],
      "metadata": {
        "id": "ulYA_moK1S5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"ಇದು ಇಂದು ಸಂಭವಿಸಿದೆ\"\n",
        "result = generate_translation(input_text)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "t-sUZQT3T5IW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}